import streamlit as st
from openai import OpenAI
from supabase import create_client
import config
import key_knowledge_generator
import answer_generator
import ground_knowledge_generator
import tail_question_decider
import tail_question_process
from predibase import Predibase
import random

# Supabase & OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
supabase = create_client(config.SUPABASE_URL, config.SUPABASE_API_KEY)
client = OpenAI(api_key=config.OPENAI_API_KEY)
pb = Predibase(api_token=config.PREDIBASE_API_TOKEN)

predeibase_model_system_message = """
ë‹¹ì‹ ì€ ëŒ€í•™ ì‹ ì…ìƒì„ ëŒ€ìƒìœ¼ë¡œ í•œ íŒŒì´ì¬ ê¸°ì´ˆ ìˆ˜ì—…ì˜ ì¡°êµ(TA) ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” AIì…ë‹ˆë‹¤. í•™ìƒë“¤ì´ ì‰½ê³  íš¨ê³¼ì ìœ¼ë¡œ íŒŒì´ì¬ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ì„¸ìš”.

1. ê°œë… ì„¤ëª…
- ì‰¬ìš´ ì–¸ì–´ë¡œ ì„¤ëª…í•˜ê³ , ì–´ë ¤ìš´ ìš©ì–´ëŠ” í”¼í•˜ì„¸ìš”.
- ì§ê´€ì ì¸ ë¹„ìœ ì™€ ì˜ˆì œë¥¼ í™œìš©í•˜ì—¬ ê°œë…ì„ ì„¤ëª…í•˜ì„¸ìš”.
- í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ìš”ì•½í•˜ê³ , ì¶”ê°€ì ì¸ ì„¤ëª…ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.

2. ë¬¸ë²• ë° ì½”ë“œ ì˜ˆì œ ì œê³µ
- ë¬¸ë²• ì„¤ëª… ì‹œ ê°„ë‹¨í•œ ì½”ë“œ ì˜ˆì œì™€ í•¨ê»˜ ì œê³µí•˜ì„¸ìš”.
- ì‹¤í–‰ ê²°ê³¼ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì—¬ í•™ìŠµìê°€ ì¦‰ì‹œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ì„¸ìš”.
- í•„ìš”í•  ê²½ìš° ì¶”ê°€ì ì¸ ì„¤ëª…ì„ ì œê³µí•˜ê³ , ì½”ë“œ ê°œì„  ë°©ë²•ë„ ì•ˆë‚´í•˜ì„¸ìš”

3. ì½”ë“œ ì˜¤ë¥˜ ë¶„ì„ ë° ë””ë²„ê¹… ì§€ì›
- ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”.
  1) ì˜¤ë¥˜ ë¶„ì„: ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í•´ì„í•˜ê³  ë°œìƒ ì›ì¸ì„ ì„¤ëª…í•˜ì„¸ìš”.
  2) í•´ê²° ë°©ë²•: ìˆ˜ì •ëœ ì½”ë“œì™€ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì œê³µí•˜ê³ , ìˆ˜ì • ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
  3) ì¶”ê°€ì ìœ¼ë¡œ ì¶œë ¥ í˜•ì‹ì´ ì•Œë§ì€ì§€ (ìŠ¤í˜ì´ìŠ¤ë°”, ì˜¤íƒ€, ì—”í„°, ì¸ë´í…Œì´ì…˜ ë“±ë“±)ë¥¼ ê²€ì‚¬í•˜ë¼ê³  ë‹¤ì‹œ í•œë²ˆ í•™ìƒì—ê²Œ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.

4. í•™ìŠµ ì§€ì› ë° ì¶”ê°€ ê¸°ëŠ¥
- í•™ìƒë“¤ì´ ì—°ìŠµí•  ìˆ˜ ìˆë„ë¡ ê´€ë ¨ëœ í•™ìŠµ ìë£Œ(íŠœí† ë¦¬ì–¼, ì—°ìŠµ ë¬¸ì œ ë“±)ë¥¼ ì œê³µí•˜ì„¸ìš”.
- ì½”ë“œ ìŠ¤íƒ€ì¼ ê°œì„  ë°©ë²•ì„ ì•ˆë‚´í•˜ì„¸ìš”.
- í•™ìƒì´ ë¶€ë‹´ ì—†ì´ ì§ˆë¬¸í•  ìˆ˜ ìˆë„ë¡ ì¹œì ˆí•œ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.
- ì§ˆë¬¸ì´ ëª¨í˜¸í•  ê²½ìš° ì¶”ê°€ ì§ˆë¬¸ì„ ìœ ë„í•˜ì—¬ í•™ìƒì´ ì›í•˜ëŠ” ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ì„¸ìš”.

"""

def coding_question_process_one(txt):
    prompt = (
        f"<|im_start|>system\n{predeibase_model_system_message}<|im_end|>\n"
        f"<|im_start|>user\n{txt}<|im_end|>\n<|im_start|>assistant\n"
    )
    lorax_client = pb.deployments.client("solar-1-mini-chat-240612")
    coding_answer = lorax_client.generate(prompt, adapter_id=config.PREDIBASE_MODEL_ONE_ADAPT_ID).generated_text
    return coding_answer

def coding_question_process_two(txt):
    prompt = (
        f"<|im_start|>system\n{predeibase_model_system_message}<|im_end|>\n"
        f"<|im_start|>user\n{txt}<|im_end|>\n<|im_start|>assistant\n"
    )
    lorax_client = pb.deployments.client("solar-1-mini-chat-240612")
    coding_answer = lorax_client.generate(prompt, adapter_id=config.PREDIBASE_MODEL_TWO_ADAPT_ID).generated_text
    return coding_answer

def save_chat_to_db(user_question, ai_answer, is_main_question, question_type):
    data = {
        "question": user_question,
        "answer": ai_answer,
        "main_question": is_main_question,
        "question_type": question_type
    }
    supabase.table("CONSULTING_CHAT").insert(data).execute()
    return


# Streamlit UI ì„¤ì •
st.set_page_config(page_title="AI ì¡°êµ", page_icon="ğŸ¤–")
st.title("ğŸ¤– AI ì¡°êµ")

with st.sidebar:
    st.markdown("# **âš¡ AI ì¡°êµ ì‚¬ìš© ì•ˆë‚´**")
    
    st.info(
        """
        - **ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ìµœëŒ€ 1~2ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**  
        - **ìƒˆë¡œê³ ì¹¨ ì‹œ ì±„íŒ… ë‚´ìš©ì´ ì‚¬ë¼ì§€ë‹ˆ ì£¼ì˜í•´ì£¼ì„¸ìš”!**  
        """,
        icon="ğŸ’¡",
    )
    
    st.markdown("# **ğŸ“ ì˜ˆì œ ì§ˆë¬¸**")
    st.markdown(
        """
        - ğŸ¯ **"ì´ë²ˆ ì½”ë“¤ ê³¼ì œëŠ” ê¸°í•œì´ ì–¸ì œê¹Œì§€ì•¼?"**  
        - ğŸ“Œ **"ì¶œì„ ê¸°ì¤€ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"**  
        - ğŸ” **"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë‘ íŠœí”Œ ì°¨ì´ê°€ ë­ì•¼?"**  
        
        ğŸ’¬ *ë‹µë³€ì´ ëª¨í˜¸í•˜ë©´ ì¶”ê°€ë¡œ ê¼¬ë¦¬ ì§ˆë¬¸ì„ í•  ìˆ˜ë„ ìˆì–´ìš”!*
        """
    )

# ì±—ë´‡ ìƒíƒœ ê´€ë¦¬
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []  # ëŒ€í™” ê¸°ë¡ ì €ì¥
if "prev_question" not in st.session_state:
    st.session_state.prev_question = None
if "prev_ground_knowledge" not in st.session_state:
    st.session_state.prev_ground_knowledge = None
if "prev_answer" not in st.session_state:
    st.session_state.prev_answer = None

# AI ì¡°êµì˜ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def chat_process(txt):
    key_knowledge = key_knowledge_generator.key_knowledge_generator(txt)
    question_type = ground_knowledge_generator.question_type_classifier(key_knowledge)
    ground_knowledge = ground_knowledge_generator.ground_knowledge_generator(key_knowledge)

    # "í•´ë‹¹ì—†ìŒ"ì¼ ê²½ìš° íŠ¹ì • ë©”ì‹œì§€ ë°˜í™˜
    if ground_knowledge.strip() == "í•´ë‹¹ì—†ìŒ":
        return ground_knowledge, "âŒ í•´ë‹¹ ì§ˆë¬¸ì€ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹´ë‹¹ ì¡°êµë‹˜ í˜¹ì€ êµìˆ˜ë‹˜ê»˜ ì§ì ‘ ë¬¸ì˜í•´ë³´ì„¸ìš”.", "í•´ë‹¹ì—†ìŒ"
    elif ground_knowledge.strip() == "ì½”ë”©":
        coding_functions = [coding_question_process_one, coding_question_process_two]
        
        # ëœë¤ìœ¼ë¡œ ì„ íƒ í›„ ì‹¤í–‰, ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ë¥¸ í•¨ìˆ˜ ì‹¤í–‰
        random.shuffle(coding_functions)  # ëœë¤ìœ¼ë¡œ ìˆœì„œ ì„ê¸°
        for func in coding_functions:
            try:
                answer = func(txt)
                return ground_knowledge, answer, "ì½”ë”©"
            except Exception as e:
                return
        return "ì½”ë”©", answer_generator.answer_generator(txt, ground_knowledge), "ì½”ë”©"
    else:
        answer = answer_generator.answer_generator(txt, ground_knowledge)
        return ground_knowledge, answer, question_type
    
# ì±„íŒ… UI ì¶œë ¥
for sender, message in st.session_state.chat_history:
    with st.chat_message("user" if sender == "user" else "assistant"):
        st.markdown(message)

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

# ì‚¬ìš©ìê°€ ì…ë ¥í–ˆì„ ë•Œ ì‹¤í–‰
if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì±„íŒ…ì°½ì— ì¶”ê°€
    st.session_state.chat_history.append(("user", user_input))
    is_main_question = True

    with st.chat_message("user"):
        st.markdown(user_input)
    
    with st.chat_message("assistant"):
        placeholder = st.empty()
        placeholder.markdown("AI ì¡°êµê°€ ìƒê° ì¤‘ì…ë‹ˆë‹¤... ğŸ¤”")
    
    if st.session_state.prev_question:  # ì´ì „ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ê¼¬ë¦¬ ì§ˆë¬¸ ì—¬ë¶€ í™•ì¸
        is_tail_question = tail_question_decider.tail_question_generator(st.session_state.prev_question, user_input)
        
        if is_tail_question.lower() == "yes":
            is_main_question = False
            assistant_response = tail_question_process.tail_question_process(
                st.session_state.prev_question,
                st.session_state.prev_ground_knowledge,
                st.session_state.prev_answer,
                user_input
            )
        else:
            # ìƒˆë¡œìš´ ì§ˆë¬¸ ì²˜ë¦¬
            is_main_question = True
            ground_knowledge, assistant_response, question_type = chat_process(user_input)
            
            # "í•´ë‹¹ì—†ìŒ"ì¼ ê²½ìš° ì²˜ë¦¬ (ë” ì§„í–‰í•˜ì§€ ì•ŠìŒ)
            if assistant_response.startswith("âŒ"):
                st.session_state.chat_history.append(("ai", assistant_response))
                st.session_state.prev_question = user_input  # ì´ì „ ì§ˆë¬¸ ì—…ë°ì´íŠ¸
                save_chat_to_db(user_input, assistant_response, True, question_type)
                st.rerun()  # UI ìƒˆë¡œê³ ì¹¨
            else:
                st.session_state.prev_ground_knowledge = ground_knowledge
                st.session_state.prev_answer = assistant_response
    else:
        # ì²« ì§ˆë¬¸ ì²˜ë¦¬
        ground_knowledge, assistant_response, question_type = chat_process(user_input)
        
        # "í•´ë‹¹ì—†ìŒ"ì¼ ê²½ìš° ì²˜ë¦¬ (ë” ì§„í–‰í•˜ì§€ ì•ŠìŒ)
        if assistant_response.startswith("âŒ"):
            st.session_state.chat_history.append(("ai", assistant_response))
            st.session_state.prev_question = user_input  # ì´ì „ ì§ˆë¬¸ ì—…ë°ì´íŠ¸
            save_chat_to_db(user_input, assistant_response, True, question_type)
            st.rerun()  # UI ìƒˆë¡œê³ ì¹¨
        else:
            st.session_state.prev_ground_knowledge = ground_knowledge
            st.session_state.prev_answer = assistant_response
    
    placeholder.markdown(assistant_response)
    
    save_chat_to_db(user_input, assistant_response, is_main_question, question_type)

    # AI ì‘ë‹µì„ ì±„íŒ…ì°½ì— ì¶”ê°€
    st.session_state.chat_history.append(("ai", assistant_response))

    # í˜„ì¬ ì§ˆë¬¸ì„ ì´ì „ ì§ˆë¬¸ìœ¼ë¡œ ì €ì¥
    st.session_state.prev_question = user_input


